{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as3091/IITJ/blob/NER/ML/Assign_2/NER/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LJz_m8KBByu"
      },
      "source": [
        "Title: Named Entity Recognition\n",
        "\n",
        "Description:\n",
        "In this NER-focused project, you will design and develop a custom Named Entity Recognition (NER) system for text analysis. Named Entity Recognition involves identifying and classifying specific entities, such as names, dates, locations, and more, within unstructured text data. Your project will offer a versatile NER solution that will work well on the provided dataset.\n",
        "\n",
        "Dataset: Named Entity Recognition (NER) Corpus (kaggle.com)\n",
        "\n",
        "https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:43:27.954258Z",
          "iopub.status.busy": "2025-04-08T06:43:27.954071Z",
          "iopub.status.idle": "2025-04-08T06:43:36.402523Z",
          "shell.execute_reply": "2025-04-08T06:43:36.401044Z",
          "shell.execute_reply.started": "2025-04-08T06:43:27.954237Z"
        },
        "id": "sIOfipnGBByw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow --quiet\n",
        "# !pip install keras --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:43:36.403769Z",
          "iopub.status.busy": "2025-04-08T06:43:36.403514Z",
          "iopub.status.idle": "2025-04-08T06:43:58.766901Z",
          "shell.execute_reply": "2025-04-08T06:43:58.766018Z",
          "shell.execute_reply.started": "2025-04-08T06:43:36.403742Z"
        },
        "id": "5YUItkm-BByx",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-13 12:52:30.792704: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-13 12:52:30.812073: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-13 12:52:30.960820: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-13 12:52:31.106999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744548751.294798   11599 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744548751.333751   11599 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1744548751.582001   11599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744548751.582058   11599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744548751.582059   11599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744548751.582061   11599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-13 12:52:31.609027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import warnings,sys, ast, pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import display, HTML\n",
        "# import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, Input, Bidirectional, LSTM, Embedding, Dropout\n",
        "from keras.models import Model\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "# from keras.random import SeedGenerator\n",
        "\n",
        "# seed_gen = SeedGenerator(seed=42)\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:43:58.770499Z",
          "iopub.status.busy": "2025-04-08T06:43:58.770237Z",
          "iopub.status.idle": "2025-04-08T06:44:02.246367Z",
          "shell.execute_reply": "2025-04-08T06:44:02.244933Z",
          "shell.execute_reply.started": "2025-04-08T06:43:58.770475Z"
        },
        "id": "XlhvMq8SBByy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install kagglehub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:02.247993Z",
          "iopub.status.busy": "2025-04-08T06:44:02.247702Z",
          "iopub.status.idle": "2025-04-08T06:44:14.598820Z",
          "shell.execute_reply": "2025-04-08T06:44:14.598058Z",
          "shell.execute_reply.started": "2025-04-08T06:44:02.247964Z"
        },
        "id": "gqChwkT_BByy",
        "outputId": "a0e1413b-1b83-4bb6-edf3-b7f7b74344f3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# while True:\n",
        "try:\n",
        "    NER_df = pd.read_csv(\"ner.csv\")\n",
        "except FileNotFoundError:\n",
        "    import kagglehub\n",
        "    from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "    # Set the path to the file you'd like to load\n",
        "    file_path = \"ner.csv\"\n",
        "\n",
        "    # Load the latest version\n",
        "    NER_df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"naseralqaydeh/named-entity-recognition-ner-corpus\",\n",
        "    file_path,\n",
        "\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:14.600166Z",
          "iopub.status.busy": "2025-04-08T06:44:14.599922Z",
          "iopub.status.idle": "2025-04-08T06:44:14.624403Z",
          "shell.execute_reply": "2025-04-08T06:44:14.623061Z",
          "shell.execute_reply.started": "2025-04-08T06:44:14.600143Z"
        },
        "id": "pbmp-QJeBByy",
        "outputId": "65ccf3ad-6000-463d-ea83-6f7791c6ef29",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "0",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "1",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "864a1043-73ce-4e6d-81e5-6ec464f67dfd",
              "rows": [
                [
                  "Sentence #",
                  "Sentence: 1",
                  "Sentence: 2"
                ],
                [
                  "Sentence",
                  "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .",
                  "Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \""
                ],
                [
                  "POS",
                  "['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']",
                  "['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', 'VBD', 'DT', 'NNS', 'WP', 'VBD', 'NNS', 'IN', 'JJ', 'NNS', 'IN', '``', 'NNP', 'NN', 'CD', 'NN', '``', 'CC', '``', 'VB', 'DT', 'NNS', '.', '``']"
                ],
                [
                  "Tag",
                  "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']",
                  "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 4
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Sentence #</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Sentence: 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sentence</th>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>Families of soldiers killed in the conflict jo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>POS</th>\n",
              "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
              "      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tag</th>\n",
              "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                            0  \\\n",
              "Sentence #                                        Sentence: 1   \n",
              "Sentence    Thousands of demonstrators have marched throug...   \n",
              "POS         ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
              "Tag         ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...   \n",
              "\n",
              "                                                            1  \n",
              "Sentence #                                        Sentence: 2  \n",
              "Sentence    Families of soldiers killed in the conflict jo...  \n",
              "POS         ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...  \n",
              "Tag         ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(NER_df.head(2).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83DhBOH3BByz"
      },
      "source": [
        "Essential info about entities:\n",
        "\n",
        "- geo = Geographical Entity\n",
        "- org = Organization\n",
        "- per = Person\n",
        "- gpe = Geopolitical Entity\n",
        "- tim = Time indicator\n",
        "- art = Artifact\n",
        "- eve = Event\n",
        "- nat = Natural Phenomenon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPUEdKJgBByz"
      },
      "source": [
        "### Class it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:14.625623Z",
          "iopub.status.busy": "2025-04-08T06:44:14.625361Z",
          "iopub.status.idle": "2025-04-08T06:44:14.629903Z",
          "shell.execute_reply": "2025-04-08T06:44:14.628612Z",
          "shell.execute_reply.started": "2025-04-08T06:44:14.625597Z"
        },
        "id": "MHkeijwKBByz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class The_Neural_Net:\n",
        "    def __init__(self):\n",
        "        self.max_len = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfdhiTYxBBy0"
      },
      "source": [
        "1. Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:14.631026Z",
          "iopub.status.busy": "2025-04-08T06:44:14.630740Z",
          "iopub.status.idle": "2025-04-08T06:44:14.641056Z",
          "shell.execute_reply": "2025-04-08T06:44:14.639637Z",
          "shell.execute_reply.started": "2025-04-08T06:44:14.630999Z"
        },
        "id": "CpILzdjKBBy0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_data(self):\n",
        "    try:\n",
        "        NER_df = pd.read_csv(\"ner.csv\")\n",
        "    except FileNotFoundError:\n",
        "        import kagglehub\n",
        "        from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "        file_path = \"ner.csv\"\n",
        "\n",
        "        NER_df = kagglehub.load_dataset(\n",
        "        KaggleDatasetAdapter.PANDAS,\n",
        "        \"naseralqaydeh/named-entity-recognition-ner-corpus\",\n",
        "        file_path,\n",
        "\n",
        "        )\n",
        "    print(NER_df.shape,\"\\n\")\n",
        "    print(\"\\n\",NER_df.info())\n",
        "    return NER_df\n",
        "The_Neural_Net.read_data = read_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP4EEJrsBBy0"
      },
      "source": [
        "2. Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:14.642208Z",
          "iopub.status.busy": "2025-04-08T06:44:14.641902Z",
          "iopub.status.idle": "2025-04-08T06:44:14.653336Z",
          "shell.execute_reply": "2025-04-08T06:44:14.651987Z",
          "shell.execute_reply.started": "2025-04-08T06:44:14.642174Z"
        },
        "id": "TkOkLpkyBBy1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def PreProcess(self):\n",
        "    NER_df = self.read_data()\n",
        "    NER_df.dropna(inplace=True)\n",
        "    NER_df.drop(columns=[\"Sentence #\",\"POS\"],inplace=True)\n",
        "    NER_df[\"Tag\"] = NER_df[\"Tag\"].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(NER_df[\"Sentence\"], NER_df[\"Tag\"], shuffle=True,test_size=0.20, random_state=42)\n",
        "    del NER_df\n",
        "\n",
        "    self.X_tokenizer = Tokenizer(lower=False,oov_token=\"UNK\")\n",
        "    self.X_tokenizer.fit_on_texts(self.X_train)\n",
        "\n",
        "    self.X_train = self.X_tokenizer.texts_to_sequences(self.X_train)\n",
        "    self.X_test = self.X_tokenizer.texts_to_sequences(self.X_test)\n",
        "\n",
        "    self.vocab_len = len(self.X_tokenizer.word_index)\n",
        "    print(f\"Number of unique tokens:\\t{self.vocab_len}\")\n",
        "\n",
        "    self.y_tokenizer = Tokenizer(lower=False,oov_token=\"UNK\")\n",
        "    self.y_tokenizer.fit_on_texts(self.y_train)\n",
        "\n",
        "    self.y_train = self.y_tokenizer.texts_to_sequences(self.y_train)\n",
        "    self.y_test = self.y_tokenizer.texts_to_sequences(self.y_test)\n",
        "\n",
        "    for dataset in [self.X_train,self.X_test]:\n",
        "        for i in range(len(dataset)):\n",
        "            self.max_len = max(self.max_len,len(dataset[i]))\n",
        "\n",
        "    self.X_train = pad_sequences(self.X_train, maxlen=self.max_len, padding=\"post\", value=0)\n",
        "    self.X_test = pad_sequences(self.X_test, maxlen=self.max_len, padding=\"post\", value=0)\n",
        "\n",
        "    self.y_train = pad_sequences(self.y_train, maxlen=self.max_len, padding=\"post\", value=0)\n",
        "    self.y_test = pad_sequences(self.y_test, maxlen=self.max_len, padding=\"post\", value=0)\n",
        "\n",
        "    self.Number_of_classes_K = len(self.y_tokenizer.word_index) + 1\n",
        "\n",
        "The_Neural_Net.PreProcess = PreProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:14.654421Z",
          "iopub.status.busy": "2025-04-08T06:44:14.654127Z",
          "iopub.status.idle": "2025-04-08T06:44:14.666538Z",
          "shell.execute_reply": "2025-04-08T06:44:14.665585Z",
          "shell.execute_reply.started": "2025-04-08T06:44:14.654396Z"
        },
        "id": "ikcuUmJBBBy1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def model_arch(self):\n",
        "    vector_size = 128\n",
        "\n",
        "    input_layer = Input(shape=(self.max_len,))\n",
        "    embedding_layer = Embedding(input_dim=self.vocab_len + 1, output_dim=vector_size, mask_zero=True, trainable=True)(input_layer)\n",
        "    dropout_layer_1 = Dropout(0.075)(embedding_layer)\n",
        "    bidirectional_LSTM_Layer = Bidirectional(LSTM(vector_size * 2, return_sequences=True))(dropout_layer_1)\n",
        "    output_layer = Dense(self.Number_of_classes_K)(bidirectional_LSTM_Layer)\n",
        "\n",
        "    self.model = Model(input_layer, output_layer)\n",
        "    print(self.model.summary())\n",
        "\n",
        "    self.model.compile(optimizer=\"adam\",loss=SparseCategoricalCrossentropy(from_logits=True),metrics=[\"accuracy\"])\n",
        "The_Neural_Net.model_arch = model_arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:14.667598Z",
          "iopub.status.busy": "2025-04-08T06:44:14.667390Z",
          "iopub.status.idle": "2025-04-08T06:44:14.680788Z",
          "shell.execute_reply": "2025-04-08T06:44:14.679744Z",
          "shell.execute_reply.started": "2025-04-08T06:44:14.667578Z"
        },
        "id": "bVM55ti6BBy1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def model_fit(self,num_of_epochs):\n",
        "    early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Metric to monitor (e.g., validation loss)\n",
        "    patience=3,          # Number of epochs with no improvement after which training will stop\n",
        "    restore_best_weights=True  # Restore the weights of the best epoch\n",
        "    )\n",
        "    self.num_of_epochs = num_of_epochs\n",
        "    self.model.fit(\n",
        "            self.X_train,\n",
        "            self.y_train,\n",
        "            epochs=num_of_epochs,\n",
        "            validation_data=(self.X_test, self.y_test),\n",
        "           # callbacks=[early_stopping]  # Include EarlyStopping in callbacks\n",
        "        )\n",
        "The_Neural_Net.model_fit = model_fit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dGj44_ZPBynu"
      },
      "outputs": [],
      "source": [
        "def save_to_file(self):\n",
        "    with open('X_tokenizer.pkl', 'wb') as file:\n",
        "        pickle.dump(self.X_tokenizer, file)\n",
        "    print(\"X_tokenizer saved to X_tokenizer.pkl\")\n",
        "\n",
        "    with open('y_tokenizer.pkl', 'wb') as file:\n",
        "        pickle.dump(self.y_tokenizer, file)\n",
        "    print(\"y_tokenizer saved to y_tokenizer.pkl\")\n",
        "\n",
        "    model_save_path = f\"ner_model_{self.num_of_epochs}.keras\"\n",
        "    # print(model_save_path)\n",
        "    self.model.save(model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "The_Neural_Net.save_to_file = save_to_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YI_cn5TpBynu"
      },
      "outputs": [],
      "source": [
        "def load_from_file(self,num_of_epochs):\n",
        "    # self.num_of_epochs = num_of_epochs\n",
        "    with open('X_tokenizer.pkl', 'rb') as file:\n",
        "        self.X_tokenizer = pickle.load(file)\n",
        "        print(\"Tokenizer loaded from X_tokenizer.pkl\")\n",
        "\n",
        "    with open('y_tokenizer.pkl', 'rb') as file:\n",
        "        self.y_tokenizer = pickle.load(file)\n",
        "        print(\"Tokenizer loaded from y_tokenizer.pkl\")\n",
        "\n",
        "    model_save_path = f\"ner_model_{num_of_epochs}.keras\"\n",
        "    self.model = load_model(model_save_path)\n",
        "    print(f\"Model loaded from {model_save_path}\")\n",
        "\n",
        "\n",
        "The_Neural_Net.load_from_file = load_from_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:14.681995Z",
          "iopub.status.busy": "2025-04-08T06:44:14.681669Z",
          "iopub.status.idle": "2025-04-08T06:44:14.692250Z",
          "shell.execute_reply": "2025-04-08T06:44:14.691223Z",
          "shell.execute_reply.started": "2025-04-08T06:44:14.681970Z"
        },
        "id": "5eIWnD-5BBy1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def predict(self,model,sentence):\n",
        "    sentence_tokens = self.X_tokenizer.texts_to_sequences([sentence])\n",
        "    # print(len(sentence.split()))\n",
        "    # print(len(sentence_tokens[0]))\n",
        "    # tokens_to_words = [word for word, index in self.X_tokenizer.word_index.items() if index in sentence_tokens[0]]\n",
        "\n",
        "    predictions = model.predict(pad_sequences(sentence_tokens,\n",
        "                                            maxlen=self.max_len,\n",
        "                                            padding=\"post\"))\n",
        "    # print(predictions)\n",
        "    prediction_ner = np.argmax(predictions,axis=-1)\n",
        "    # print(prediction_ner)\n",
        "\n",
        "    NER_tags = [self.y_tokenizer.index_word[num] for num in list(prediction_ner.flatten())]\n",
        "    final_pred = {\"Word\":[],\"Tag\":[]}\n",
        "    sentence_split = sentence.split()\n",
        "    for Word,Tag in zip(sentence_split,NER_tags):\n",
        "        # final_pred[tokens_to_words[i]] = NER_tags[i]\n",
        "        final_pred[\"Word\"].append(Word)\n",
        "        final_pred[\"Tag\"].append(Tag)\n",
        "    return pd.DataFrame(final_pred)\n",
        "The_Neural_Net.predict = predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:14.693439Z",
          "iopub.status.busy": "2025-04-08T06:44:14.693135Z",
          "iopub.status.idle": "2025-04-08T06:44:19.662284Z",
          "shell.execute_reply": "2025-04-08T06:44:19.661152Z",
          "shell.execute_reply.started": "2025-04-08T06:44:14.693414Z"
        },
        "id": "4_aarSViBBy2",
        "outputId": "074b68cd-d6ca-4e45-dddd-4673b9eaa33b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(47959, 4) \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 47959 entries, 0 to 47958\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Sentence #  47959 non-null  object\n",
            " 1   Sentence    47959 non-null  object\n",
            " 2   POS         47959 non-null  object\n",
            " 3   Tag         47959 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 1.5+ MB\n",
            "\n",
            " None\n",
            "Number of unique tokens:\t28761\n"
          ]
        }
      ],
      "source": [
        "NN_obj = The_Neural_Net()\n",
        "NN_obj.PreProcess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "execution": {
          "iopub.execute_input": "2025-04-08T06:44:28.651413Z",
          "iopub.status.busy": "2025-04-08T06:44:28.651115Z",
          "iopub.status.idle": "2025-04-08T06:46:40.867517Z",
          "shell.execute_reply": "2025-04-08T06:46:40.865866Z",
          "shell.execute_reply.started": "2025-04-08T06:44:28.651387Z"
        },
        "id": "b1f3yDKaDRd6",
        "outputId": "20bab25d-3449-4b6e-d30b-aa48ea5552a1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
            "INFO:tensorflow:Initializing the TPU system: local\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,681,536</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">788,480</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">89</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,747</span> │ bidirectional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m3,681,536\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m788,480\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m89\u001b[0m, \u001b[38;5;34m19\u001b[0m)    │      \u001b[38;5;34m9,747\u001b[0m │ bidirectional_2[\u001b[38;5;34m…\u001b[0m │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,479,763</span> (17.09 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,479,763\u001b[0m (17.09 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,479,763</span> (17.09 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,479,763\u001b[0m (17.09 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Epoch 1/50\n",
            "\u001b[1m1199/1199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 254ms/step - accuracy: 0.2160 - loss: 0.6814 - val_accuracy: 0.2206 - val_loss: 0.3185\n",
            "Epoch 2/50\n",
            "\u001b[1m1199/1199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 237ms/step - accuracy: 0.2232 - loss: 0.2918 - val_accuracy: 0.2234 - val_loss: 0.2809\n",
            "Epoch 3/50\n",
            "\u001b[1m1199/1199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 240ms/step - accuracy: 0.2274 - loss: 0.2247 - val_accuracy: 0.2244 - val_loss: 0.2773\n",
            "Epoch 4/50\n",
            "\u001b[1m1199/1199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 232ms/step - accuracy: 0.2305 - loss: 0.1859 - val_accuracy: 0.2247 - val_loss: 0.2909\n",
            "Epoch 5/50\n",
            "\u001b[1m1199/1199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 237ms/step - accuracy: 0.2329 - loss: 0.1556 - val_accuracy: 0.2246 - val_loss: 0.3029\n",
            "Epoch 6/50\n",
            "\u001b[1m1199/1199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 230ms/step - accuracy: 0.2347 - loss: 0.1339 - val_accuracy: 0.2243 - val_loss: 0.3188\n",
            "Epoch 7/50\n",
            "\u001b[1m1199/1199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 249ms/step - accuracy: 0.2361 - loss: 0.1150 - val_accuracy: 0.2244 - val_loss: 0.3427\n",
            "Epoch 8/50\n",
            "\u001b[1m1199/1199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 252ms/step - accuracy: 0.2376 - loss: 0.0976 - val_accuracy: 0.2241 - val_loss: 0.3656\n",
            "Epoch 9/50\n",
            "\u001b[1m 463/1199\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:55\u001b[0m 238ms/step - accuracy: 0.2389 - loss: 0.0840"
          ]
        }
      ],
      "source": [
        "num_of_epochs = 50\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local') # Detect TPU\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "    num_of_epochs = 100\n",
        "    with tpu_strategy.scope():\n",
        "        NN_obj.model_arch()\n",
        "        NN_obj.model_fit(num_of_epochs)\n",
        "\n",
        "except Exception as e:\n",
        "    NN_obj.model_arch()\n",
        "    NN_obj.model_fit(num_of_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:46:40.870108Z",
          "iopub.status.busy": "2025-04-08T06:46:40.869789Z",
          "iopub.status.idle": "2025-04-08T06:46:40.874844Z",
          "shell.execute_reply": "2025-04-08T06:46:40.873621Z",
          "shell.execute_reply.started": "2025-04-08T06:46:40.870078Z"
        },
        "id": "-gP2rBVgBBy2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# num_of_epochs = 5\n",
        "# if tf.test.is_gpu_available():\n",
        "#     num_of_epochs = 100\n",
        "#     with tf.device('/device:GPU:0'):\n",
        "#         NN_obj.model_fit(num_of_epochs)\n",
        "# else:\n",
        "#     NN_obj.model_fit(num_of_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5myd2XyuCXEw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_tokenizer saved to X_tokenizer.pkl\n",
            "y_tokenizer saved to y_tokenizer.pkl\n",
            "Model saved to ner_model_50.keras\n"
          ]
        }
      ],
      "source": [
        "NN_obj.save_to_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iItmWwcOBynv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded from X_tokenizer.pkl\n",
            "Tokenizer loaded from y_tokenizer.pkl\n",
            "Model loaded from ner_model_50.keras\n"
          ]
        }
      ],
      "source": [
        "NN_obj.load_from_file(num_of_epochs=num_of_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:46:40.876082Z",
          "iopub.status.busy": "2025-04-08T06:46:40.875803Z",
          "iopub.status.idle": "2025-04-08T06:46:43.265881Z",
          "shell.execute_reply": "2025-04-08T06:46:43.264566Z",
          "shell.execute_reply.started": "2025-04-08T06:46:40.876056Z"
        },
        "id": "ON0FZkoaBBy2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Word",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Tag",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "88a3d96f-3ada-49b6-abd2-43b0aa6fbdfd",
              "rows": [
                [
                  "0",
                  "Is",
                  "O"
                ],
                [
                  "1",
                  "this",
                  "O"
                ],
                [
                  "2",
                  "the",
                  "O"
                ],
                [
                  "3",
                  "real",
                  "O"
                ],
                [
                  "4",
                  "life?",
                  "O"
                ],
                [
                  "5",
                  "Is",
                  "O"
                ],
                [
                  "6",
                  "this",
                  "O"
                ],
                [
                  "7",
                  "just",
                  "O"
                ],
                [
                  "8",
                  "fantasy?",
                  "O"
                ],
                [
                  "9",
                  "Caught",
                  "O"
                ],
                [
                  "10",
                  "in",
                  "O"
                ],
                [
                  "11",
                  "a",
                  "O"
                ],
                [
                  "12",
                  "landslide,",
                  "O"
                ],
                [
                  "13",
                  "no",
                  "O"
                ],
                [
                  "14",
                  "escape",
                  "O"
                ],
                [
                  "15",
                  "from",
                  "O"
                ],
                [
                  "16",
                  "reality",
                  "O"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 17
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Is</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>real</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>life?</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Is</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>this</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>just</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fantasy?</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Caught</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>in</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>a</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>landslide,</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>no</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>escape</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>reality</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Word Tag\n",
              "0           Is   O\n",
              "1         this   O\n",
              "2          the   O\n",
              "3         real   O\n",
              "4        life?   O\n",
              "5           Is   O\n",
              "6         this   O\n",
              "7         just   O\n",
              "8     fantasy?   O\n",
              "9       Caught   O\n",
              "10          in   O\n",
              "11           a   O\n",
              "12  landslide,   O\n",
              "13          no   O\n",
              "14      escape   O\n",
              "15        from   O\n",
              "16     reality   O"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentence = \"\"\"Is this the real life? Is this just fantasy? Caught in a landslide, no escape from reality\"\"\"\n",
        "model = NN_obj.model\n",
        "prediction_df = NN_obj.predict(model=NN_obj.model,sentence=sentence)\n",
        "display(prediction_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "b5n-hXZtBynv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Word",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Tag",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "33fdabda-fe3c-43a4-ae25-11c2bc761db3",
              "rows": [
                [
                  "0",
                  "Apoorv",
                  "O"
                ],
                [
                  "1",
                  "Code,",
                  "O"
                ],
                [
                  "2",
                  "Ankur",
                  "O"
                ],
                [
                  "3",
                  "question",
                  "O"
                ],
                [
                  "4",
                  "one,",
                  "O"
                ],
                [
                  "5",
                  "Alok,",
                  "O"
                ],
                [
                  "6",
                  "zoom",
                  "I-per"
                ],
                [
                  "7",
                  "meeting",
                  "O"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 8
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Apoorv</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Code,</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ankur</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>question</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one,</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Alok,</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>zoom</td>\n",
              "      <td>I-per</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>meeting</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Word    Tag\n",
              "0    Apoorv      O\n",
              "1     Code,      O\n",
              "2     Ankur      O\n",
              "3  question      O\n",
              "4      one,      O\n",
              "5     Alok,      O\n",
              "6      zoom  I-per\n",
              "7   meeting      O"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentence = \"\"\"Apoorv Code, Ankur question one, Alok, zoom meeting\"\"\"\n",
        "model = NN_obj.model\n",
        "prediction_df = NN_obj.predict(model=NN_obj.model,sentence=sentence)\n",
        "display(prediction_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jDstqk5Bynv"
      },
      "source": [
        "# Rough work below this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:46:43.267624Z",
          "iopub.status.busy": "2025-04-08T06:46:43.267319Z",
          "iopub.status.idle": "2025-04-08T06:46:43.437310Z",
          "shell.execute_reply": "2025-04-08T06:46:43.436248Z",
          "shell.execute_reply.started": "2025-04-08T06:46:43.267594Z"
        },
        "id": "Rv0pyTx-BBy3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save the trained model to a file\n",
        "# dt_now = dt.datetime.now().strftime(\"%y%m%d_%H%M\")\n",
        "model_save_path = f\"ner_model_{num_of_epochs}.keras\"  # You can specify any file path\n",
        "# print(model_save_path)\n",
        "NN_obj.model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-08T06:46:43.438288Z",
          "iopub.status.busy": "2025-04-08T06:46:43.438046Z",
          "iopub.status.idle": "2025-04-08T06:46:44.093317Z",
          "shell.execute_reply": "2025-04-08T06:46:44.091901Z",
          "shell.execute_reply.started": "2025-04-08T06:46:43.438265Z"
        },
        "id": "6Gov5DK4BBy3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model(model_save_path)\n",
        "\n",
        "sentence = \"\"\"Is this the real life? Is this just fantasy? Caught in a landslide, no escape from reality\"\"\"\n",
        "\n",
        "prediction_df = NN_obj.predict(model=loaded_model,sentence=sentence)\n",
        "display(prediction_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bd68hW2BBy3"
      },
      "outputs": [],
      "source": [
        "sys.exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RHYZpalBBy3"
      },
      "source": [
        "# Rough work below this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fvgBG8uBBy3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3nTxw0aBBy4"
      },
      "outputs": [],
      "source": [
        "sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34xWLQ2xBBy4"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.models import Model\n",
        "\n",
        "input_layer = Input(shape=(10,))\n",
        "dense_layer = Dense(32, activation='relu')(input_layer)\n",
        "dropout_layer = Dropout(0.5)(dense_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkjVEHkJBBy4"
      },
      "source": [
        "3. Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f95uYthSBBy4"
      },
      "outputs": [],
      "source": [
        "def Tokenize(self):\n",
        "\n",
        "\n",
        "\n",
        "The_Neural_Net.PreProcess = PreProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl71PhTjBBy5"
      },
      "outputs": [],
      "source": [
        "print(NER_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDZklbuXBBy5"
      },
      "outputs": [],
      "source": [
        "print(NER_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hshsw96vBBy6"
      },
      "outputs": [],
      "source": [
        "NER_df.dropna(inplace=True)\n",
        "NER_df.drop(columns=[\"Sentence #\",\"POS\"],inplace=True)\n",
        "NER_df[\"Tag\"] = NER_df[\"Tag\"].apply(lambda x: ast.literal_eval(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV6L_uibBBy6"
      },
      "outputs": [],
      "source": [
        "print(NER_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lThaFLzmBBy6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(NER_df[\"Sentence\"], NER_df[\"Tag\"], shuffle=True,test_size=0.20, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, shuffle=True,test_size=0.50, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrCg2fD8BBy6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "X_tokenizer = Tokenizer(lower=False,oov_token=\"UNK\")\n",
        "X_tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faIbCZ_rBBy6"
      },
      "outputs": [],
      "source": [
        "X_train.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo9bszb2BBy6"
      },
      "outputs": [],
      "source": [
        "X_train = X_tokenizer.texts_to_sequences(X_train)\n",
        "X_test = X_tokenizer.texts_to_sequences(X_test)\n",
        "X_val = X_tokenizer.texts_to_sequences(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lLUJEHmBBy7"
      },
      "outputs": [],
      "source": [
        "vocab_len = len(X_tokenizer.word_index)\n",
        "print(f\"Number of unique tokens:\\t{vocab_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hXP7QrzBBy7"
      },
      "outputs": [],
      "source": [
        "train_tags = set([val for sublist in y_train for val in sublist])\n",
        "test_tags = set([val for sublist in y_test for val in sublist])\n",
        "val_tags = set([val for sublist in y_val for val in sublist])\n",
        "\n",
        "print(\"Unique NER tags in train set: \",train_tags)\n",
        "print(\"Unique NER tags in test set: \",test_tags)\n",
        "print(\"Unique NER tags in test set: \",val_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7MFkqntBBy_"
      },
      "outputs": [],
      "source": [
        "y_tokenizer = Tokenizer(lower=False,oov_token=\"UNK\")\n",
        "y_tokenizer.fit_on_texts(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndwk39paBBy_"
      },
      "outputs": [],
      "source": [
        "y_train = y_tokenizer.texts_to_sequences(y_train)\n",
        "y_test = y_tokenizer.texts_to_sequences(y_test)\n",
        "y_val = y_tokenizer.texts_to_sequences(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpgMLGefBBy_"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "for dataset in [X_train,X_test,X_val]:\n",
        "    for i in range(len(dataset)):\n",
        "        max_len = max(max_len,len(dataset[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ia8aGb_BBzA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding=\"post\", value=0)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\", value=0)\n",
        "X_val = pad_sequences(X_val, maxlen=max_len, padding=\"post\", value=0)\n",
        "\n",
        "y_train = pad_sequences(y_train, maxlen=max_len, padding=\"post\", value=0)\n",
        "y_test = pad_sequences(y_test, maxlen=max_len, padding=\"post\", value=0)\n",
        "y_val = pad_sequences(y_val, maxlen=max_len, padding=\"post\", value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5kN0ObsBBzA"
      },
      "outputs": [],
      "source": [
        "for dataset in [X_train,X_test,X_val,y_train,y_test,y_val]:\n",
        "    print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqnJq70GBBzA"
      },
      "outputs": [],
      "source": [
        "Number_of_classes_K = len(y_tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUpTqnmmBBzA"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.models import Model\n",
        "\n",
        "input_layer = Input(shape=(10,))\n",
        "dense_layer = Dense(32, activation='relu')(input_layer)\n",
        "dropout_layer = Dropout(0.5)(dense_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmNdaanbBBzA"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Input, Bidirectional, LSTM, Embedding, Dropout\n",
        "from keras.models import Model\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej6HRBszBBzA"
      },
      "outputs": [],
      "source": [
        "vector_size = 64\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=vocab_len + 1, output_dim=vector_size, mask_zero=True, trainable=True)(input_layer)\n",
        "dropout_layer_1 = Dropout(0.2)(embedding_layer)\n",
        "bidirectional_LSTM_Layer = Bidirectional(LSTM(vector_size * 2, return_sequences=True))(dropout_layer_1)\n",
        "output_layer = Dense(Number_of_classes_K)(bidirectional_LSTM_Layer)\n",
        "\n",
        "model = Model(input_layer, output_layer)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhPjiVH3BBzA"
      },
      "outputs": [],
      "source": [
        "pprint(model.get_config())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJj8FCvABBzB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlhgpaOCBBzB"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Metric to monitor (e.g., validation loss)\n",
        "    patience=3,          # Number of epochs with no improvement after which training will stop\n",
        "    restore_best_weights=True  # Restore the weights of the best epoch\n",
        ")\n",
        "model.compile(optimizer=\"adam\",loss=SparseCategoricalCrossentropy(from_logits=True),metrics=[\"accuracy\"])\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=6,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping]  # Include EarlyStopping in callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H5TGCoaBBzB"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"Is this the real life? Is this just fantasy? Caught in a landslide, no escape from reality\"\"\"\n",
        "unpadded_len = len(sentence.split(\" \"))\n",
        "predictions = model.predict(pad_sequences(X_tokenizer.texts_to_sequences([sentence]),\n",
        "                                          maxlen=max_len,\n",
        "                                         padding=\"post\"))\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOhZDwq4BBzB"
      },
      "outputs": [],
      "source": [
        "prediction_ner = np.argmax(predictions,axis=-1)\n",
        "print(prediction_ner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RlEcZU0BBzB"
      },
      "outputs": [],
      "source": [
        "NER_tags = [y_tokenizer.index_word[num] for num in list(prediction_ner.flatten())][:unpadded_len]\n",
        "print(NER_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIV0z55_BBzC"
      },
      "outputs": [],
      "source": [
        "sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJTDkLFmBBzC"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOxg0qR1BBzC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-smtIDCBBzC"
      },
      "outputs": [],
      "source": [
        "tf.test.is_gpu_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op38PT3UBBzD"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkML7-gaBBzD"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKlztf0FBBzD"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFbWj10DBBzD"
      },
      "outputs": [],
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLAAxH6XBBzD"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tDSVGrtBBzE"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47EH8pYIBBzE"
      },
      "outputs": [],
      "source": [
        "tf.device('/device:GPU:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT6VU6TkBBzE"
      },
      "outputs": [],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "    model.fit(x_train, y_train, epochs=500, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BXygTPHBBzE"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81dmAWPCBBzE"
      },
      "outputs": [],
      "source": [
        "predictions = model(x_test[:1]).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIk1Cj40BBzE"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "databundleVersionId": 3092179,
          "datasetId": 1861688,
          "sourceId": 3043695,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30920,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
