{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as3091/IITJ/blob/ML_Assign/ML/Assign_2/Assign_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4H9-TYwzwgk"
      },
      "source": [
        "Q3: Implementation of Neural Networks from Scratch Using NumPy and Comparison with Sklearn (20 marks):\n",
        "  \n",
        "    1. Load and preprocess the MNIST Digits Dataset. (3 marks)\n",
        "    2. Implement a neural network with one input layer, one hidden layer, and one output layer using NumPy. (5 marks)\n",
        "    3. Train the neural network with various hyperparameters (e.g., learning rate, number of hidden nodes). (3 marks)\n",
        "    4. Evaluate the performance of the neural network on the testing set. (2 marks)\n",
        "    5. Implement the same neural network using sklearn and compare the results with the NumPy implementation. (4 marks)\n",
        "    6. Plot the training and validation loss/accuracy curves (for both experiments). (3 marks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5fi7HVHozwgm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import warnings,gc,sys, random\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm import tqdm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from pprint import pprint\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Load & Preprocess"
      ],
      "metadata": {
        "id": "_cjAptKG55vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load and preprocess the MNIST Digits Dataset. (3 marks)"
      ],
      "metadata": {
        "id": "kR10sU7-5jLw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D0iyz9Erzwgn"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    try:\n",
        "        mnist_784_df = pd.read_csv(\"mnist_784.csv\")\n",
        "        break\n",
        "    except FileNotFoundError:\n",
        "        from sklearn.datasets import fetch_openml\n",
        "\n",
        "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "        X[\"Y\"] = y\n",
        "        X.to_csv(\"mnist_784.csv\",index=False,header=True)\n",
        "        del X,y\n",
        "# %%script echo skipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IXEsaD_Szwgo"
      },
      "outputs": [],
      "source": [
        "X = mnist_784_df.drop(columns=\"Y\").values.astype(np.float32) / 255.0\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X = scaler.fit_transform(X)\n",
        "\n",
        "Y = mnist_784_df[\"Y\"].values\n",
        "# result = np.where(targets == 5, 1, 0)\n",
        "\n",
        "IL_n = mnist_784_df.shape[-1]-1\n",
        "n_neurons_HL = int(np.sqrt(IL_n-1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Creating Object\n",
        "# scaler = StandardScaler()\n",
        "# # Standardizing the features\n",
        "# X = scaler.fit_transform(inputs)"
      ],
      "metadata": {
        "id": "XJ6t0P8Bopnp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTr9DU3zwgo"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "inputs = pd.DataFrame(scaler.fit_transform(inputs), columns=inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "svaq-NFAzwgo"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y,\n",
        "    test_size=0.20, random_state=42,\n",
        "    stratify=Y,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_index = np.random.choice(len(X_train))\n",
        "plt.imshow(X_train[random_index].reshape(28, 28), cmap='gray')\n",
        "plt.title(f'{y_train[random_index]}')\n",
        "plt.show()\n",
        "\n",
        "random_index = np.random.choice(len(X_test))\n",
        "plt.imshow(X_test[random_index].reshape(28, 28), cmap='gray')\n",
        "plt.title(f'{y_test[random_index]}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y_ohjHx6wTc4",
        "outputId": "38b5141f-9e4f-462b-d0f3-1cb67676a698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train.astype(np.float32) / 255.0\n",
        "# X_test = X_test.astype(np.float32) / 255.0\n",
        "y_train = np.eye(10)[y_train]  # One-hot encode labels\n",
        "y_test = np.eye(10)[y_test]  # One-hot encode labels"
      ],
      "metadata": {
        "id": "lVXk06ZA_NCs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Build Neural Network"
      ],
      "metadata": {
        "id": "H2Fz0oN_5wMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a neural network with one input layer, one hidden layer, and one output layer using NumPy. (5 marks)"
      ],
      "metadata": {
        "id": "llQ5UCS95sWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    log_probs = -np.log(y_pred[np.arange(m), y_true.argmax(axis=1)])\n",
        "    return float(np.sum(log_probs) / m)"
      ],
      "metadata": {
        "id": "87620kETs3Ad"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    # def __init__(self, input_size, hidden_size, output_size):\n",
        "    def __init__(self, input_size, hidden_size, output_size, batch_size , learning_rate,epochs):\n",
        "        # He initialization for ReLU\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2./input_size)\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2./hidden_size)\n",
        "        self.b2 = np.zeros(output_size)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.loss_dict={ \"epoch\":[],  \"train_loss\":[], \"test_loss\":[]}\n",
        "\n",
        "    def relu(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))  # Numerical stability\n",
        "        return exp_Z / exp_Z.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.Z1 = X.dot(self.W1) + self.b1\n",
        "        self.A1 = self.relu(self.Z1)\n",
        "        self.Z2 = self.A1.dot(self.W2) + self.b2\n",
        "        return self.softmax(self.Z2)\n",
        "\n",
        "    def backprop(self, X, y_true, y_pred):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Output layer gradient\n",
        "        dZ2 = y_pred - y_true\n",
        "        dW2 = (self.A1.T.dot(dZ2)) / m\n",
        "        db2 = np.sum(dZ2, axis=0) / m\n",
        "\n",
        "        # Hidden layer gradient\n",
        "        dA1 = dZ2.dot(self.W2.T)\n",
        "        dZ1 = dA1 * (self.Z1 > 0)  # ReLU derivative\n",
        "        dW1 = (X.T.dot(dZ1)) / m\n",
        "        db1 = np.sum(dZ1, axis=0) / m\n",
        "\n",
        "        # Update parameters\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "\n",
        "    def fit(self, X_train, y_train, loss = False, X_test = None, y_test = None ):\n",
        "\n",
        "        X_batches,y_batches = [], []\n",
        "        count=0\n",
        "        for i in range(0,X_train.shape[0], self.batch_size):\n",
        "            X_batches.append(X_train[i:i+self.batch_size])\n",
        "            y_batches.append(y_train[i:i+self.batch_size])\n",
        "            count+=1\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            # Mini-batch training\n",
        "            for i in range(count):\n",
        "\n",
        "                self.backprop(X = X_batches[i],\n",
        "                              y_true = y_batches[i],\n",
        "                              y_pred = self.forward(X_batches[i]))\n",
        "\n",
        "            # Compute epoch loss\n",
        "            if (epoch+1)%(self.epochs//10) == 0:\n",
        "                if loss:\n",
        "                    self.loss_dict[\"epoch\"].append(epoch)\n",
        "                    y_pred = self.forward(X_train)\n",
        "                    train_loss = compute_loss(y_pred = y_pred, y_true = y_train)\n",
        "\n",
        "                    # print(y_pred.shape,\"\\n\",y_pred)\n",
        "                    # print(y_train.shape,\"\\n\",y_train)\n",
        "                    # sys.exit()\n",
        "                    self.loss_dict[\"train_loss\"].append(train_loss)\n",
        "\n",
        "                    test_loss = compute_loss(y_pred = self.forward(X_test), y_true = y_test)\n",
        "                    self.loss_dict[\"test_loss\"].append(test_loss)\n",
        "            # sys.exit()\n",
        "    def predict(self,X):\n",
        "        return self.forward(X)\n",
        "\n",
        "    def score(self,X,y):\n",
        "        y_pred = self.predict(X)\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        labels = np.argmax(y, axis=1)\n",
        "        return accuracy_score(labels,predictions)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # \"\"\"Get parameters for this estimator.\"\"\"\n",
        "        return {\n",
        "            \"input_size\": self.input_size,\n",
        "            \"hidden_size\": self.hidden_size,\n",
        "            \"output_size\": self.output_size,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"epochs\": self.epochs\n",
        "        }\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        \"\"\"Set the parameters of this estimator.\"\"\"\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self"
      ],
      "metadata": {
        "id": "STaz5Vsc_vXI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Train Neural Net"
      ],
      "metadata": {
        "id": "NNGGydCR6HQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train the neural network with various hyperparameters (e.g., learning rate, number of hidden nodes). (3 marks)"
      ],
      "metadata": {
        "id": "mi5dxHur6NJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize network\n",
        "%%time\n",
        "nn = NeuralNetwork(input_size=784, output_size=10,\n",
        "                   # Hyperparameters\n",
        "                   hidden_size=300,batch_size = 128, learning_rate = 0.1,epochs = 10)\n",
        "nn.fit(X_train = X_train,y_train = y_train)\n",
        "# pprint(nn.loss_dict)"
      ],
      "metadata": {
        "id": "opcR4k0lnUHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nn.predict(X_test)\n",
        "predictions = np.argmax(y_pred, axis=1)\n",
        "labels = np.argmax(y_test, axis=1)\n",
        "print(accuracy_score(labels,predictions))"
      ],
      "metadata": {
        "id": "8tNcD4DNwhxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Performance testing"
      ],
      "metadata": {
        "id": "_L2mFUf37FSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluate the performance of the neural network on the testing set. (2 marks)"
      ],
      "metadata": {
        "id": "KOpzQSih7Ezw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(labels, predictions)\n",
        "# Get unique class labels\n",
        "unique_labels = np.unique(labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_labels) # Use unique labels\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(labels, predictions, target_names=unique_labels.astype(str))) # Use unique labels as target names"
      ],
      "metadata": {
        "id": "INqoHrmJwhpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xfvfVe8kwhff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(params):\n",
        "    # clf = RandomForestClassifier(**params)\n",
        "    nn_clf = NeuralNetwork(**params)\n",
        "    # nn = NeuralNetwork(input_size=784, output_size=10,\n",
        "    #                # Hyperparameters\n",
        "    #                hidden_size=300,batch_size = 128, learning_rate = 0.1,epochs = 20)\n",
        "    score = cross_val_score(nn_clf, X_train, y_train, cv=5).mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}"
      ],
      "metadata": {
        "id": "qpHliQ36nTzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "output_size=10\n",
        "print(int(np.sqrt(input_size+1)))"
      ],
      "metadata": {
        "id": "a3BCl2-Z0wEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "output_size=10\n",
        "space = {\n",
        "    'input_size': hp.choice('input_size', [input_size]),\n",
        "    'output_size': hp.choice('output_size', [output_size]),\n",
        "    \"epochs\": hp.choice(\"epochs\",[20]),\n",
        "\n",
        "    'hidden_size': hp.uniformint(\"hidden_size\", int(np.sqrt(input_size+1)), int((input_size+1)/2)),\n",
        "    'batch_size': hp.uniformint(\"batch_size\", int(X_train.shape[0]/1000), int(X_train.shape[0]/100)),\n",
        "    \"learning_rate\": hp.uniform('learning_rate', 0.001, 0.1),\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "js9kgS3zznTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trials = Trials()\n",
        "# best = fmin(fn=objective,\n",
        "#             space=space,\n",
        "#             algo=tpe.suggest,\n",
        "#             max_evals=10,\n",
        "#             trials=trials)\n",
        "# best_params = space_eval(space=space,hp_assignment=best)\n",
        "# print(\"Best parameters: \")\n",
        "# pprint(best_params)"
      ],
      "metadata": {
        "id": "_aGtiwkfnShW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {'batch_size': 177,\n",
        " 'epochs': 50,\n",
        " 'hidden_size': 285,\n",
        " 'input_size': 784,\n",
        " 'learning_rate': 0.08644110664186987,\n",
        " 'output_size': 10}"
      ],
      "metadata": {
        "id": "kRzUIolrEap2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_NN_clf = NeuralNetwork(**best_params)\n",
        "best_NN_clf.fit(X_train, y_train,loss=True,X_test=X_test,y_test=y_test)\n",
        "\n",
        "y_pred = best_NN_clf.predict(X_test)\n",
        "predictions = np.argmax(y_pred, axis=1)\n",
        "labels = np.argmax(y_test, axis=1)"
      ],
      "metadata": {
        "id": "juO6lVID48LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(labels,predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "cm = confusion_matrix(labels, predictions)\n",
        "# Get unique class labels\n",
        "unique_labels = np.unique(labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_labels) # Use unique labels\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(labels, predictions, target_names=unique_labels.astype(str))) # Use unique labels as target names"
      ],
      "metadata": {
        "id": "rwaNHDIY4hSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 sklearn Neural Net"
      ],
      "metadata": {
        "id": "OFfELJ5s7h_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Implement the same neural network using sklearn and compare the results with the NumPy implementation. (4 marks)"
      ],
      "metadata": {
        "id": "liX8c_km70eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_train[0])\n",
        "# print(y_pred[0])\n",
        "# print(y_train[0])\n",
        "\n",
        "# # log_probs = -np.log(y_pred[np.arange(m), y_true.argmax(axis=1)])\n",
        "# # print(y_train.arg)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JJl0SYiIutAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "# n_epochs = 50\n",
        "# train_loss, train_acc = [], []\n",
        "# val_loss, val_acc = [], []\n",
        "loss_dict={ \"epoch\":[],  \"train_loss\":[], \"test_loss\":[]}\n",
        "\n",
        "epochs = best_params[\"epochs\"]\n",
        "mlp = MLPClassifier(activation='relu', batch_size = best_params[\"batch_size\"],\n",
        "                    hidden_layer_sizes=(best_params[\"hidden_size\"]), learning_rate_init=best_params[\"learning_rate\"],learning_rate='constant',\n",
        "                    max_iter=1,\n",
        "                    # max_iter=best_params[\"epochs\"],\n",
        "                    random_state=42,\n",
        "                    warm_start=True,shuffle=False, early_stopping=False)\n",
        "\n",
        "for epoch in tqdm(range(best_params[\"epochs\"])):\n",
        "    # Train for 1 epoch\n",
        "    mlp.fit(X_train, y_train)\n",
        "    loss_dict[\"epoch\"].append(epoch)\n",
        "    # Training metrics\n",
        "    # print(mlp.loss_curve_)\n",
        "    # loss_dict[\"train_loss\"].append(mlp.loss_curve_[-1])           # Training loss\n",
        "    # train_acc.append(model.score(X_train, y_train))    # Training accuracy\n",
        "\n",
        "    if (epoch+1)%(epochs//10) == 0:\n",
        "        # loss_dict[\"epoch\"].append(epoch)\n",
        "\n",
        "        train_loss = compute_loss(y_pred = mlp.predict_proba(X_train), y_true = y_train)\n",
        "        loss_dict[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        test_loss = compute_loss(y_pred = mlp.predict_proba(X_test), y_true = y_test)\n",
        "        loss_dict[\"test_loss\"].append(test_loss)\n",
        "        # print(f\"{train_loss:.2f} \\t {test_loss:.2f}\")\n",
        "\n",
        "# Training the model\n",
        "mlp.fit(X_train, y_train)\n",
        "# Making prediction\n",
        "y_pred = mlp.predict_proba(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "pprint(loss_dict)"
      ],
      "metadata": {
        "id": "PRgF8kIw6-2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.exit()"
      ],
      "metadata": {
        "id": "CmRpSWbsu9c3",
        "outputId": "0ce78395-65fe-4f02-de2e-ea9517fef6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Plot the training and validation loss/accuracy curves (for both experiments). (3 marks)"
      ],
      "metadata": {
        "id": "WFNHXFTa73pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "# n_epochs = 50\n",
        "# train_loss, train_acc = [], []\n",
        "# val_loss, val_acc = [], []\n",
        "loss_dict={ \"epoch\":[],  \"train_loss\":[], \"test_loss\":[]}\n",
        "\n",
        "epochs = best_params[\"epochs\"]\n",
        "mlp = MLPClassifier(activation='relu', batch_size = best_params[\"batch_size\"],\n",
        "                    hidden_layer_sizes=(best_params[\"hidden_size\"]), learning_rate_init=best_params[\"learning_rate\"],learning_rate='constant',\n",
        "                    max_iter=1,\n",
        "                    # max_iter=best_params[\"epochs\"],\n",
        "                    random_state=42,\n",
        "                    warm_start=True,shuffle=False, early_stopping=False)\n",
        "\n",
        "for epoch in tqdm(range(best_params[\"epochs\"])):\n",
        "    # Train for 1 epoch\n",
        "    mlp.fit(X_train, y_train)\n",
        "    loss_dict[\"epoch\"].append(epoch)\n",
        "    # Training metrics\n",
        "    # loss_dict[\"train_loss\"].append(mlp.loss_curve_[-1])           # Training loss\n",
        "    # train_acc.append(model.score(X_train, y_train))    # Training accuracy\n",
        "\n",
        "\n",
        "    # y_pred = mlp.predict_proba(X_train)\n",
        "    # predictions = np.argmax(y_pred, axis=1)\n",
        "    # labels = np.argmax(y_train, axis=1)\n",
        "    # accuracy = accuracy_score(labels,predictions)\n",
        "    # print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    # print(y_pred.shape,\"\\n\",y_pred)\n",
        "    # print(y_train.shape,\"\\n\",y_train)\n",
        "    # print(predictions)\n",
        "    # print(labels)\n",
        "    # print(log_loss(y_test, y_pred))\n",
        "    # print(compute_loss(y_pred, y_train))\n",
        "\n",
        "    # Validation metrics\n",
        "    # y_pred = mlp.predict_proba(X_test)\n",
        "    # predictions = np.argmax(y_pred, axis=1)\n",
        "    # labels = np.argmax(y_test, axis=1)\n",
        "    # accuracy = accuracy_score(labels,predictions)\n",
        "    # print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    # print(y_pred.shape,\"\\n\",y_pred)\n",
        "    # print(y_test.shape,\"\\n\",y_test)\n",
        "    # print(predictions)\n",
        "    # print(labels)\n",
        "    # # print(log_loss(y_test, y_pred))\n",
        "    # print(compute_loss(y_pred, y_test))\n",
        "    # loss_dict[\"test_loss\"].append(log_loss(y_val, y_val_proba))\n",
        "    # val_loss.append(log_loss(y_val, y_val_proba))      # Validation loss\n",
        "    # val_acc.append(model.score(X_val, y_val))          # Validation accuracy\n",
        "    # print(mlp.predict(X_train),y_train)\n",
        "\n",
        "    # y_pred = mlp.predict(X_train)\n",
        "\n",
        "    # print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    # sys.exit()\n",
        "    if (epoch+1)%(epochs//10) == 0:\n",
        "        loss_dict[\"epoch\"].append(epoch)\n",
        "\n",
        "        train_loss = compute_loss(y_pred = mlp.predict(X_train), y_true = y_train)\n",
        "        loss_dict[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        test_loss = compute_loss(y_pred = mlp.predict(X_test), y_true = y_test)\n",
        "        loss_dict[\"test_loss\"].append(test_loss)\n",
        "\n",
        "\n",
        "# Training the model\n",
        "mlp.fit(X_train, y_train)\n",
        "# Making prediction\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "pprint(loss_dict)"
      ],
      "metadata": {
        "id": "VerLfsuc-yvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly"
      ],
      "metadata": {
        "id": "Uzu68bC8u8rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "FZtPQWS_9uTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X-axis values (shared across all lines)\n",
        "x = np.arange(2000, 2020)\n",
        "\n",
        "# Y-axis values for four lines\n",
        "y1 = np.random.randint(50, 100, len(x))  # Line 1\n",
        "y2 = np.random.randint(30, 80, len(x))   # Line 2\n",
        "y3 = np.random.randint(10, 60, len(x))   # Line 3\n",
        "y4 = np.random.randint(20, 90, len(x))   # Line 4\n"
      ],
      "metadata": {
        "id": "HIDmX8Uq-Glv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create figure\n",
        "fig = go.Figure()\n",
        "# Add traces (lines)\n",
        "fig.add_trace(go.Scatter(x=x, y=y1, mode='lines', name='Line 1'))\n",
        "fig.add_trace(go.Scatter(x=x, y=y2, mode='lines', name='Line 2'))\n",
        "fig.add_trace(go.Scatter(x=x, y=y3, mode='lines', name='Line 3'))\n",
        "fig.add_trace(go.Scatter(x=x, y=y4, mode='lines', name='Line 4'))\n"
      ],
      "metadata": {
        "id": "NbkL8pGf-JTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title=\"Line Chart with Four Lines\",\n",
        "    xaxis_title=\"Year\",\n",
        "    yaxis_title=\"Values\",\n",
        "    legend_title=\"Lines\",\n",
        "    template=\"plotly_dark\"\n",
        ")\n",
        "\n",
        "# Show plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "AMsthE6c-JGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ht9BkcO29eaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QGh98q69eNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j3G-xCfzwgo"
      },
      "outputs": [],
      "source": [
        "mlp.loss_curve_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krzMmnmszwgn"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize network\n",
        "nn = NeuralNetwork(input_size=784, output_size=10)\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "learning_rate = 0.1\n",
        "hidden_size=300\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle data\n",
        "    permutation = np.random.permutation(X_train.shape[0])\n",
        "    X_shuffled = X_train[permutation]\n",
        "    y_shuffled = y_train[permutation]\n",
        "\n",
        "    # Mini-batch training\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_shuffled[i:i+batch_size]\n",
        "        y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = nn.forward(X_batch)\n",
        "\n",
        "        # Backpropagation\n",
        "        nn.backprop(X_batch, y_batch, y_pred, learning_rate)\n",
        "\n",
        "    # Compute epoch loss\n",
        "    y_pred = nn.forward(X_train)\n",
        "    loss = nn.compute_loss(y_pred, y_train)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "jOGqBIGb_vZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U48Y4cTCOBot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_accuracy(X, y):\n",
        "y_pred = nn.forward(X_test)\n",
        "predictions = np.argmax(y_pred, axis=1)\n",
        "labels = np.argmax(y_test, axis=1)\n",
        "print(accuracy_score(labels,predictions))\n",
        "# return np.mean(predictions == labels)\n",
        "\n",
        "# print(f\"\\nTraining Accuracy: {compute_accuracy(X_train, y_train):.4f}\")\n",
        "# print(f\"Test Accuracy: {compute_accuracy(X_test, y_test):.4f}\")\n"
      ],
      "metadata": {
        "id": "xbbFk2VE_vcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(labels, predictions)\n",
        "# Get unique class labels\n",
        "unique_labels = np.unique(labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_labels) # Use unique labels\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(labels, predictions, target_names=unique_labels.astype(str))) # Use unique labels as target names"
      ],
      "metadata": {
        "id": "brtUHoYPBpHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%script false --no-raise-error\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "eiLLfADw_ve4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-2KvBvX8_vhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IynwkxdU_vjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yVmSs2yzwgo"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim,inputs, targets):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets.reshape(-1,1)\n",
        "        # print(self.targets.shape)\n",
        "\n",
        "        self.weights1 = np.random.rand(input_dim, hidden_dim)\n",
        "        self.weights2 = np.random.rand(hidden_dim, output_dim)\n",
        "        self.bias1 = np.zeros((1, hidden_dim))\n",
        "        self.bias2 = np.zeros((1, output_dim))\n",
        "\n",
        "    def forward_propagation(self):\n",
        "\n",
        "        hidden_layer_input = np.dot(self.inputs, self.weights1) + self.bias1\n",
        "        self.hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "        output_layer_input = np.dot(self.hidden_layer_output, self.weights2) + self.bias2\n",
        "        self.output_layer_output = sigmoid(output_layer_input)\n",
        "        # print(self.output_layer_output.shape)\n",
        "        # sys.exit()\n",
        "        # return hidden_layer_output, output_layer_output\n",
        "\n",
        "    def backward_propagation(self):\n",
        "        # , targets, hidden_layer_output, output_layer_output):\n",
        "\n",
        "        d_output = 2 * (self.output_layer_output - self.targets)\n",
        "        self.d_weights2 = np.dot(self.hidden_layer_output.T, d_output * sigmoid_derivative(self.output_layer_output))\n",
        "        self.d_bias2 = np.sum(d_output * sigmoid_derivative(self.output_layer_output), axis=0, keepdims=True)\n",
        "\n",
        "        d_hidden_layer = np.dot(d_output * sigmoid_derivative(self.output_layer_output), self.weights2.T) * relu_derivative(self.hidden_layer_output)\n",
        "        self.d_weights1 = np.dot(self.inputs.T, d_hidden_layer)\n",
        "        self.d_bias1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
        "\n",
        "        # return d_weights1, d_bias1, d_weights2, d_bias2\n",
        "\n",
        "    def update_weights(self,learning_rate):\n",
        "        # , d_weights1, d_bias1, d_weights2, d_bias2, learning_rate):\n",
        "        self.weights1 -= learning_rate * self.d_weights1\n",
        "        self.bias1 -= learning_rate * self.d_bias1\n",
        "        self.weights2 -= learning_rate * self.d_weights2\n",
        "        self.bias2 -= learning_rate * self.d_bias2\n",
        "\n",
        "    def runner(self, epochs, learning_rate):\n",
        "        # inputs, targets, epochs, learning_rate\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            # hidden_layer_output, output_layer_output = self.forward_propagation(inputs)\n",
        "            # d_weights1, d_bias1, d_weights2, d_bias2 = self.backward_propagation(inputs, targets, hidden_layer_output, output_layer_output)\n",
        "            # self.update_weights(d_weights1, d_bias1, d_weights2, d_bias2, learning_rate)\n",
        "\n",
        "            self.forward_propagation()\n",
        "            self.backward_propagation()\n",
        "            self.update_weights(learning_rate=learning_rate)\n",
        "\n",
        "                # Print loss every 1000 epochs\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(self.output_layer_output - self.targets))\n",
        "                print(f\"\\nEpoch {epoch+1}, Loss: {loss}\")\n",
        "                gc.collect()\n",
        "\n",
        "        # _, final_output =\n",
        "        self.forward_propagation(self.inputs)\n",
        "        print(\"Final Predictions:\")\n",
        "        print(self.output_layer_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-sdgAIizwgp"
      },
      "outputs": [],
      "source": [
        "nn = NeuralNetwork(input_dim=IL_n, hidden_dim=n_neurons_HL, output_dim=1,inputs=inputs,targets=targets)\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "nn.runner(epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.exit()"
      ],
      "metadata": {
        "id": "mcBrCmOZz-z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a726cixezwgp"
      },
      "outputs": [],
      "source": [
        "nn = NeuralNetwork(input_dim=IL_n, hidden_dim=n_neurons_HL, output_dim=1,input=inputs,targets=targets)\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    hidden_layer_output, output_layer_output = nn.forward_propagation(inputs)\n",
        "    d_weights1, d_bias1, d_weights2, d_bias2 = nn.backward_propagation(inputs, targets, hidden_layer_output, output_layer_output)\n",
        "    nn.update_weights(d_weights1, d_bias1, d_weights2, d_bias2, learning_rate)\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(output_layer_output - targets))\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
        "\n",
        "# Final predictions\n",
        "_, final_output = nn.forward_propagation(inputs)\n",
        "print(\"Final Predictions:\")\n",
        "print(final_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPVz0TpRzwgq"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self,value):\n",
        "        self.value = value\n",
        "class hidden_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ro2I9jRzwgq"
      },
      "outputs": [],
      "source": [
        "class aHL:\n",
        "    def __init__(self,mnist_784_df):\n",
        "\n",
        "\n",
        "    def forward_prop_relu(self,input,weight):\n",
        "        return np.maximum(0, input*weight)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_uJ6esozwgq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZzGoFwEzwgq"
      },
      "outputs": [],
      "source": [
        "X.to_csv(\"mnist_784.csv\",index=False,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZXE9q0Dzwgq"
      },
      "outputs": [],
      "source": [
        "display(X.head().T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwc70PtQzwgq"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(X, columns=[f'Pixel_{i}'for i in range(X.shape[-1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o49d8VNhzwgq"
      },
      "outputs": [],
      "source": [
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_SCsMTzzwgq"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAYQJN6Czwgq"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim,inputs, targets):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "        self.weights1 = np.random.rand(input_dim, hidden_dim)\n",
        "        self.weights2 = np.random.rand(hidden_dim, output_dim)\n",
        "        self.bias1 = np.zeros((1, hidden_dim))\n",
        "        self.bias2 = np.zeros((1, output_dim))\n",
        "\n",
        "    def forward_propagation(self):\n",
        "\n",
        "        hidden_layer_input = np.dot(self.inputs, self.weights1) + self.bias1\n",
        "        hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "        output_layer_input = np.dot(hidden_layer_output, self.weights2) + self.bias2\n",
        "        output_layer_output = sigmoid(output_layer_input)\n",
        "\n",
        "        return hidden_layer_output, output_layer_output\n",
        "\n",
        "    def backward_propagation(self, targets, hidden_layer_output, output_layer_output):\n",
        "\n",
        "        d_output = 2 * (output_layer_output - targets)\n",
        "        d_weights2 = np.dot(hidden_layer_output.T, d_output * sigmoid_derivative(output_layer_output))\n",
        "        d_bias2 = np.sum(d_output * sigmoid_derivative(output_layer_output), axis=0, keepdims=True)\n",
        "\n",
        "        d_hidden_layer = np.dot(d_output * sigmoid_derivative(output_layer_output), self.weights2.T) * relu_derivative(hidden_layer_output)\n",
        "        d_weights1 = np.dot(self.inputs.T, d_hidden_layer)\n",
        "        d_bias1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
        "\n",
        "        return d_weights1, d_bias1, d_weights2, d_bias2\n",
        "\n",
        "    def update_weights(self, d_weights1, d_bias1, d_weights2, d_bias2, learning_rate):\n",
        "        self.weights1 -= learning_rate * d_weights1\n",
        "        self.bias1 -= learning_rate * d_bias1\n",
        "        self.weights2 -= learning_rate * d_weights2\n",
        "        self.bias2 -= learning_rate * d_bias2\n",
        "\n",
        "    def runner(self, epochs, learning_rate):\n",
        "        # inputs, targets, epochs, learning_rate\n",
        "        for i in range(epochs):\n",
        "            # hidden_layer_output, output_layer_output = self.forward_propagation(inputs)\n",
        "            # d_weights1, d_bias1, d_weights2, d_bias2 = self.backward_propagation(inputs, targets, hidden_layer_output, output_layer_output)\n",
        "            # self.update_weights(d_weights1, d_bias1, d_weights2, d_bias2, learning_rate)\n",
        "\n",
        "            self.forward_propagation()\n",
        "            self.backward_propagation()\n",
        "            self.update_weights()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Loss: {np.mean(np.square(output_layer_output - targets))}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}